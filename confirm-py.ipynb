{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyterをpyにするためのチェック用\n",
    "\n",
    "sigmaを1から20にする\n",
    "\n",
    "'state'を'pref'に置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as sps\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "GAMMA = 1/7\n",
    "R_T_MAX = 12\n",
    "r_t_range = np.linspace(0, R_T_MAX, R_T_MAX*100+1)\n",
    "\n",
    "def get_posteriors(sr, sigma=0.15):\n",
    "\n",
    "    # (1) Calculate Lambda\n",
    "    lam = sr[:-1].values * np.exp(GAMMA * (r_t_range[:, None] - 1))\n",
    "    lam += 10e-6\n",
    "\n",
    "    # (2) Calculate each day's likelihood\n",
    "    likelihoods = pd.DataFrame(\n",
    "        data = sps.poisson.pmf(sr[1:].values, lam),\n",
    "        index = r_t_range,\n",
    "        columns = sr.index[1:])\n",
    "\n",
    "    # (3) Create the Gaussian Matrix\n",
    "    process_matrix = sps.norm(loc=r_t_range,\n",
    "                              scale=sigma\n",
    "                             ).pdf(r_t_range[:, None])\n",
    "\n",
    "    # (3a) Normalize all rows to sum to 1\n",
    "    process_matrix /= process_matrix.sum(axis=0)\n",
    "\n",
    "    # (4) Calculate the initial prior\n",
    "    prior0 = sps.gamma(a=4).pdf(r_t_range)\n",
    "    prior0 /= prior0.sum()\n",
    "\n",
    "    # Create a DataFrame that will hold our posteriors for each day\n",
    "    # Insert our prior as the first posterior.\n",
    "    posteriors = pd.DataFrame(\n",
    "        index=r_t_range,\n",
    "        columns=sr.index,\n",
    "        data={sr.index[0]: prior0}\n",
    "    )\n",
    "\n",
    "    # We said we'd keep track of the sum of the log of the probability\n",
    "    # of the data for maximum likelihood calculation.\n",
    "    log_likelihood = 0.0\n",
    "\n",
    "    # (5) Iteratively apply Bayes' rule\n",
    "    for previous_day, current_day in zip(sr.index[:-1], sr.index[1:]):\n",
    "\n",
    "        #(5a) Calculate the new prior\n",
    "        current_prior = process_matrix @ posteriors[previous_day]\n",
    "\n",
    "        #(5b) Calculate the numerator of Bayes' Rule: P(k|R_t)P(R_t)\n",
    "        numerator = likelihoods[current_day] * current_prior\n",
    "\n",
    "        #(5c) Calcluate the denominator of Bayes' Rule P(k)\n",
    "        denominator = np.sum(numerator)\n",
    "\n",
    "        # Execute full Bayes' Rule\n",
    "        posteriors[current_day] = numerator/denominator\n",
    "\n",
    "        # Add to the running sum of log likelihoods\n",
    "        log_likelihood += np.log(denominator)\n",
    "\n",
    "    return posteriors, log_likelihood\n",
    "\n",
    "\n",
    "def prepare_cases(cases, state_name, latest, cutoff=0):\n",
    "\n",
    "    new_cases = cases.diff()\n",
    "\n",
    "    # fill until latest\n",
    "    state = new_cases.xs(state_name)\n",
    "    if latest not in state:\n",
    "        new_cases = new_cases.append(pd.Series({(state_name, latest): 0}))\n",
    "\n",
    "    # fill NaN\n",
    "    first = new_cases.index[0]\n",
    "    new_cases.loc[first] = cases[0]\n",
    "\n",
    "    # fill 0\n",
    "    new_cases = new_cases.unstack(level=[0]).asfreq('D', fill_value=0).stack(level=[0]).swaplevel(1,0)\n",
    "\n",
    "    std = 2\n",
    "    window = 7\n",
    "    if new_cases.values.max() < 5:\n",
    "#         window = 3\n",
    "        std = 0.1\n",
    "    elif new_cases.values.max() < 25:\n",
    "        window = 5\n",
    "#         std = 1\n",
    "\n",
    "    smoothed = new_cases.rolling(window,\n",
    "        win_type='gaussian',\n",
    "        min_periods=1,\n",
    "        center=True).mean(std=std).round()\n",
    "\n",
    "    idx_start = np.searchsorted(smoothed, cutoff)\n",
    "\n",
    "    smoothed = smoothed.iloc[idx_start:]\n",
    "    original = new_cases.loc[smoothed.index]\n",
    "\n",
    "    return original, smoothed\n",
    "\n",
    "\n",
    "def highest_density_interval(pmf, p=.9):\n",
    "    # If we pass a DataFrame, just call this recursively on the columns\n",
    "    if(isinstance(pmf, pd.DataFrame)):\n",
    "        return pd.DataFrame([highest_density_interval(pmf[col], p=p) for col in pmf],\n",
    "                            index=pmf.columns)\n",
    "\n",
    "    cumsum = np.cumsum(pmf.values)\n",
    "\n",
    "    # N x N matrix of total probability mass for each low, high\n",
    "    total_p = cumsum - cumsum[:, None]\n",
    "\n",
    "    # Return all indices with total_p > p\n",
    "    lows, highs = (total_p > p).nonzero()\n",
    "\n",
    "    # Find the smallest range (highest density)\n",
    "    best = (highs - lows).argmin()\n",
    "\n",
    "    low = pmf.index[lows[best]]\n",
    "    high = pmf.index[highs[best]]\n",
    "\n",
    "    return pd.Series([low, high],\n",
    "                     index=[f'Low_{p*100:.0f}',\n",
    "                            f'High_{p*100:.0f}'])\n",
    "\n",
    "\n",
    "url = 'https://dl.dropboxusercontent.com/s/6mztoeb6xf78g5w/COVID-19.csv'\n",
    "data = pd.read_csv(url,\n",
    "                     usecols=['確定日', '受診都道府県', '人数'],\n",
    "                     parse_dates=['確定日'],\n",
    "                    ).sort_index()\n",
    "\n",
    "FILTERED_REGION = ['岩手県']\n",
    "PREF =  ['北海道', '青森県', '宮城県', '秋田県', '山形県', '福島県',\n",
    "        '茨城県', '栃木県', '群馬県', '埼玉県', '千葉県', '東京都', '神奈川県',\n",
    "        '新潟県', '富山県', '石川県', '福井県', '山梨県', '長野県', '岐阜県',\n",
    "        '静岡県', '愛知県', '三重県', '滋賀県', '京都府', '大阪府', '兵庫県',\n",
    "        '奈良県', '和歌山県', '鳥取県', '島根県', '岡山県', '広島県', '山口県',\n",
    "        '徳島県', '香川県', '愛媛県', '高知県', '福岡県', '佐賀県', '長崎県',\n",
    "        '熊本県', '大分県', '宮崎県', '鹿児島県', '沖縄県']\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "for pref in PREF:\n",
    "    ds = data.groupby(['確定日', '受診都道府県'], as_index=False).sum()\n",
    "    dsp = ds[ds['受診都道府県']==pref]\n",
    "    dsp['累積人数'] = dsp['人数'].cumsum()\n",
    "    dsp = dsp.rename(columns={'確定日': 'date', '受診都道府県': 'pref'})\n",
    "    dspi = dsp.set_index(['pref', 'date'])\n",
    "    df_all = pd.concat([df_all, dspi])\n",
    "states = df_all['累積人数']\n",
    "\n",
    "\n",
    "\n",
    "# Choosing the optimal sigma\n",
    "sigmas = np.linspace(1/20, 1, 20)\n",
    "\n",
    "targets = ~states.index.get_level_values('pref').isin(FILTERED_REGION)\n",
    "states_to_process = states.loc[targets]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for state_name, cases in states_to_process.groupby(level='pref'):\n",
    "\n",
    "    tokyo = states.xs('東京都')\n",
    "    latest = tokyo.index[-1]\n",
    "\n",
    "    print(state_name)\n",
    "    new, smoothed = prepare_cases(cases, state_name, latest, cutoff=0)\n",
    "\n",
    "#     if len(smoothed) == 0:\n",
    "#         new, smoothed = prepare_cases(cases, cutoff=0)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    # Holds all posteriors with every given value of sigma\n",
    "    result['posteriors'] = []\n",
    "\n",
    "    # Holds the log likelihood across all k for each value of sigma\n",
    "    result['log_likelihoods'] = []\n",
    "\n",
    "    for sigma in sigmas:\n",
    "        posteriors, log_likelihood = get_posteriors(smoothed, sigma=sigma)\n",
    "        result['posteriors'].append(posteriors)\n",
    "        result['log_likelihoods'].append(log_likelihood)\n",
    "\n",
    "    # Store all results keyed off of state name\n",
    "    results[state_name] = result\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "total_log_likelihoods = np.zeros_like(sigmas)\n",
    "\n",
    "# Loop through each state's results and add the log likelihoods to the running total.\n",
    "for state_name, result in results.items():\n",
    "    total_log_likelihoods += result['log_likelihoods']\n",
    "\n",
    "# Select the index with the largest log likelihood total\n",
    "max_likelihood_index = total_log_likelihoods.argmax()\n",
    "\n",
    "# Select the value that has the highest log likelihood\n",
    "sigma = sigmas[max_likelihood_index]\n",
    "print(f'Maximum Likelihood value for sigma = {sigma:.2f}')\n",
    "\n",
    "\n",
    "# Compile Final Results\n",
    "final_results = None\n",
    "\n",
    "for state_name, result in results.items():\n",
    "    print(state_name)\n",
    "    posteriors = result['posteriors'][max_likelihood_index]\n",
    "    hdis_90 = highest_density_interval(posteriors, p=.9)\n",
    "    hdis_50 = highest_density_interval(posteriors, p=.5)\n",
    "    most_likely = posteriors.idxmax().rename('ML')\n",
    "    result = pd.concat([most_likely, hdis_90, hdis_50], axis=1)\n",
    "    if final_results is None:\n",
    "        final_results = result\n",
    "    else:\n",
    "        final_results = pd.concat([final_results, result])\n",
    "\n",
    "print('Done.')\n",
    "\n",
    "# Export\n",
    "final_results.to_csv('rt_japan.csv', float_format='%.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.dates import date2num, num2date\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rt(result, ax, state_name):\n",
    "    \n",
    "    ax.set_title(f\"{state_name}\")\n",
    "    \n",
    "    # Colors\n",
    "    ABOVE = [1,0,0]\n",
    "    MIDDLE = [1,1,1]\n",
    "    BELOW = [0,0,0]\n",
    "    cmap = ListedColormap(np.r_[\n",
    "        np.linspace(BELOW,MIDDLE,25),\n",
    "        np.linspace(MIDDLE,ABOVE,25)\n",
    "    ])\n",
    "    color_mapped = lambda y: np.clip(y, .5, 1.5)-.5\n",
    "    \n",
    "    index = result['ML'].index.get_level_values('date')\n",
    "    values = result['ML'].values\n",
    "    \n",
    "    # Plot dots and line\n",
    "    ax.plot(index, values, c='k', zorder=1, alpha=.25)\n",
    "    ax.scatter(index,\n",
    "               values,\n",
    "               s=40,\n",
    "               lw=.5,\n",
    "               c=cmap(color_mapped(values)),\n",
    "               edgecolors='k', zorder=2)\n",
    "    \n",
    "    # Aesthetically, extrapolate credible interval by 1 day either side\n",
    "    lowfn = interp1d(date2num(index),\n",
    "                     result['Low_90'].values,\n",
    "                     bounds_error=False,\n",
    "                     fill_value='extrapolate')\n",
    "    \n",
    "    highfn = interp1d(date2num(index),\n",
    "                      result['High_90'].values,\n",
    "                      bounds_error=False,\n",
    "                      fill_value='extrapolate')\n",
    "    \n",
    "    extended = pd.date_range(start=pd.Timestamp('2020-03-01'),\n",
    "                             end=index[-1]+pd.Timedelta(days=1))\n",
    "    \n",
    "    ax.fill_between(extended,\n",
    "                    lowfn(date2num(extended)),\n",
    "                    highfn(date2num(extended)),\n",
    "                    color='k',\n",
    "                    alpha=.1,\n",
    "                    lw=0,\n",
    "                    zorder=3)\n",
    "\n",
    "    ax.axhline(1.0, c='k', lw=1, label='$R_t=1.0$', alpha=.25);\n",
    "    \n",
    "    # Formatting\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "    ax.xaxis.set_minor_locator(mdates.DayLocator())\n",
    "    \n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter(\"{x:.1f}\"))\n",
    "    ax.yaxis.tick_right()\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.margins(0)\n",
    "    ax.grid(which='major', axis='y', c='k', alpha=.1, zorder=-2)\n",
    "    ax.margins(0)\n",
    "    ax.set_ylim(0.0, 5.0)\n",
    "    ax.set_xlim(pd.Timestamp('2020-03-01'), result.index.get_level_values('date')[-1]+pd.Timedelta(days=1))\n",
    "    fig.set_facecolor('w')\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(600/72,400/72))\n",
    "\n",
    "plot_rt(result, ax, state_name)\n",
    "ax.set_title(f'Real-time $R_t$ for {state_name}')\n",
    "ax.xaxis.set_major_locator(mdates.WeekdayLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = int(np.ceil(len(results) / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows*3))\n",
    "\n",
    "for i, (state_name, result) in enumerate(final_results.groupby('state')):\n",
    "    plot_rt(result, axes.flat[i], state_name)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor('w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
